# Detect to Track and Track to Detect

**Object Detection.** Two families of detectors are currently popular: First, region proposal based detectors R-CNN, Fast R-CNN, Faster R-CNN and R-FCN and second, detectors that directly predict boxes for an image in one step such as YOLO and SSD.

Our approach builds on R-FCN which is a simple and efficient framework for object detection on region proposals with a fully convolutional nature. In terms of accuracy it is competitive with Faster R-CNN which uses a multilayer network that is evaluated per-region (and thus has a **cost growing linearly with the number of candidate RoIs**). R-FCN reduces the cost for region classification by pushing the region-wise operations to the end of the network with the introduction of a position-sensitive RoI pooling layer which works on convolutional features that **encode the spatially subsampled class scores of input RoIs**.



**Tracking.** Tracking is also an extensively studied problem in computer vision with most recent progress devoted to trackers operating on deep ConvNet features. In [*Learning multi-domain convolutional neural networks for visual tracking. In Proc. CVPR, 2016.*] a ConvNet is fine-tuned at test-time to track a target from the same video via detection and bounding box regression. Training on the examples of a test sequence is slow and also not applicable in the object detection setting. Other methods use pre-trained ConvNet features to track and have achieved strong performance either with correlation [1, 25] or regression trackers on heat maps [39] or bounding boxes [13].



**Video Object Detection.** Action detection is also a related problem and has received increased attention recently, mostly with methods building on two-stream ConvNets [35]. In [11] a method is presented that uses a two-stream R-CNN [10] to classify regions and link them across frames based on the action predictions and their spatial overlap. This method has been adopted by [33] and [27] where the R-CNN was replaced by Faster R-CNN with the RPN operating on two streams of appearance and motion information.

One area of interest is learning to detect and localize in each frame (e.g. in video co-localization) with only weak supervision. The YouTube Object Dataset [28], has been used for this purpose, e.g. [15, 20].

Since the object detection from video task has been introduced at the ImageNet challenge, it has drawn significant attention. 

```[TCNN]``` In [18] tubelet proposals are generated by **applying a tracker to frame-based bounding box proposals**. The detector scores across the video are re-scored by a 1D CNN model. In their corresponding ILSVRC submission the group [17] added a propagation of scores to nearby frames based on optical flows between frames and suppression of class scores that are not among the top classes in a video. A more recent work [16] introduces a tubelet proposal network that regresses static object proposals over multiple frames, extracts features by applying Faster R-CNN which are finally processed by an encoder-decoder LSTM.

```[DFF]``` In deep feature flow [42] a recognition ConvNet is applied to key frames only and an optical flow ConvNet is used for propagating the deep feature maps via a flow field to the rest of the frames. This approach can increase detection speed by a factor of 5 at a slight accuracy cost. The approach is error-prone due largely to two aspects: First, propagation from the key frame to the current frame can be erroneous and, second, the key frames can miss features from current frames. Very recently a new large-scale dataset for video object detection has been introduced [29] with single objects annotations over video sequences.

# Flow-Guided Feature Aggregation for Video Object Detection

**Object detection in video.** Recently, ImageNet introduces a new challenge for object detection from videos (VID), which brings object detection into the video domain. In this challenge, nearly all of existing methods incorporate temporal information only on the final stage “ boundingbox post-processing”.

```[TCNN]``` T-CNN [18, 19] propagates predicted bounding boxes to neighboring frames according to precomputed optical flows, and then generates tubelets by applying tracking algorithms from high-confidence bounding boxes. Boxes along the tubelets are re-scored based on tubelets classification.

```[Seq-NMS]``` Seq-NMS [12] constructs sequences along nearby high-confidence bounding boxes from consecutive frames. Boxes of the sequence are re-scored to the average confidence, other boxes close to this sequence are suppressed.

Unfortunately, all of these methods are multi-stage pipeline, where results in each stage would rely on the results from previous stages. Thus, it is difficult to correct errors produced by previous stages.

By contrast, our method considers temporal information at the feature level instead of the final box level. The entire system is end-to-end trained for the task of video object detection. Besides, our method can further incorporate such bounding-box post-processing techniques to improve the recognition accuracy.