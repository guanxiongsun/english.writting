# [D&T]Detect to Track and Track to Detect

**Object Detection.** Two families of detectors are currently popular: First, region proposal based detectors R-CNN, Fast R-CNN, Faster R-CNN and R-FCN and second, detectors that directly predict boxes for an image in one step such as YOLO and SSD.

Our approach builds on R-FCN which is a simple and efficient framework for object detection on region proposals with a fully convolutional nature. In terms of accuracy it is competitive with Faster R-CNN which uses a multilayer network that is evaluated per-region (and thus has a **cost growing linearly with the number of candidate RoIs**). R-FCN reduces the cost for region classification by pushing the region-wise operations to the end of the network with the introduction of a position-sensitive RoI pooling layer which works on convolutional features that **encode the spatially subsampled class scores of input RoIs**.



**Tracking.** Tracking is also an extensively studied problem in computer vision with most recent progress devoted to trackers operating on deep ConvNet features. In [*Learning multi-domain convolutional neural networks for visual tracking. In Proc. CVPR, 2016.*] a ConvNet is fine-tuned at test-time to track a target from the same video via detection and bounding box regression. Training on the examples of a test sequence is slow and also not applicable in the object detection setting. Other methods use pre-trained ConvNet features to track and have achieved strong performance either with correlation [1, 25] or regression trackers on heat maps [39] or bounding boxes [13].



**Video Object Detection.** Action detection is also a related problem and has received increased attention recently, mostly with methods building on two-stream ConvNets [35]. In [11] a method is presented that uses a two-stream R-CNN [10] to classify regions and link them across frames based on the action predictions and their spatial overlap. This method has been adopted by [33] and [27] where the R-CNN was replaced by Faster R-CNN with the RPN operating on two streams of appearance and motion information.

One area of interest is learning to detect and localize in each frame (e.g. in video co-localization) with only weak supervision. The YouTube Object Dataset [28], has been used for this purpose, e.g. [15, 20].

Since the object detection from video task has been introduced at the ImageNet challenge, it has drawn significant attention. 

```[TCNN]``` In [18] tubelet proposals are generated by **applying a tracker to frame-based bounding box proposals**. The detector scores across the video are **re-scored** by a 1D CNN model. In their corresponding ILSVRC submission the group [17] added a propagation of scores to nearby frames based on optical flows between frames and suppression of class scores that are not among the top classes in a video. A more recent work [16] introduces a tubelet proposal network that regresses static object proposals over multiple frames, extracts features by applying Faster R-CNN which are finally processed by an encoder-decoder LSTM.

```[DFF]``` In deep feature flow [42] a recognition ConvNet is applied to key frames only and an optical flow ConvNet is used for propagating the deep feature maps via a flow field to the rest of the frames. This approach can increase detection speed by a factor of 5 at a slight accuracy cost. The approach is error-prone due largely to two aspects: First, propagation from the key frame to the current frame can be erroneous and, second, the key frames can miss features from current frames. Very recently a new large-scale dataset for video object detection has been introduced [29] with single objects annotations over video sequences.

## Reference

[16] K. Kang. Object detection in videos with tubelet proposal networks. In Proc. CVPR, 2017.

[17] K. Kang. T-CNN: tubelets with convolutional neural networks for object detection from videos. arXiv preprint, 2016.

[18] K. Kang, W. Ouyang, H. Li. Object detection from video tubelets with convolutional neural networks. In

Proc. CVPR, 2016.

# [FGFA]Flow-Guided Feature Aggregation for Video Object Detection

**Object detection in video.** Recently, ImageNet introduces a new challenge for object detection from videos (VID), which brings object detection into the video domain. In this challenge, nearly all of existing methods incorporate temporal information only on the final stage “ boundingbox post-processing”.

```[TCNN]``` **T-CNN** [18, 19] propagates predicted bounding boxes to neighboring frames according to precomputed optical flows, and then generates tubelets by applying tracking algorithms from high-confidence bounding boxes. Boxes along the tubelets are re-scored based on tubelets classification.

```[Seq-NMS]``` **Seq-NMS** [12] constructs sequences along nearby high-confidence bounding boxes from consecutive frames. Boxes of the sequence are re-scored to the average confidence, other boxes close to this sequence are suppressed.

Unfortunately, all of these methods are multi-stage pipeline, where results in each stage would rely on the results from previous stages. Thus, it is difficult to correct errors produced by previous stages.

By contrast, our method considers temporal information at the feature level instead of the final box level. The entire system is end-to-end trained for the task of video object detection. Besides, our method can further incorporate such bounding-box post-processing techniques to improve the recognition accuracy.

# [Cycle-consist]Learning Correspondence from the Cycle-consistency of Time

**Tracking.** Classic approaches to tracking treat it as a matching problem, where the goal is to find a given object/patch in the next frame (see [11] for overview), and the key challenge is to track reliably over extended time periods [57, 79, 1, 27]. Starting with the seminal work of Ramanan et al. [49], researchers largely turned to “tracking as repeated recognition”, where trained object detectors are applied to each frame independently [3, 28, 80, 71, 19, 35, 65]. Our work harks back to the classic tracking-by-matching methods in treating it as a correspondence problem, but uses learning to obtain a robust representation that is able to model wide range of appearance changes.

# [ICCV_Memory]Object Guided External Memory Network for Video Object Detection

**Video Object Detection.** The task of video object detection aims to detect every frame of a video. Box level methods [19, 20, 12, 8, 3, 24] optimize the bounding box linkage across multiple frames. T-CNN [19, 20] leverages optical flow to propagate the bounding box across frames and links the bounding box into tubelets with the help of tracking algorithms. Seq-NMS [12] considers all bounding boxes within a video and re-scores them for optimal bounding box linkage. DTTD [8] simultaneously achieves detections and trackings. The frame level detections are linked by across-frame tracklets to provide the final prediction. STL [3] detects on sparse key frames, and propagates the predicted bounding box to non-key frames through motion and scales. DorT [24] combines detection and tracking for efficient detection.

# [MANet]Fully Motion-Aware Network for Video Object Detection

**Post-processing.** the main idea is to use high-scoring objects from nearby frames to boost scores of weaker detections within the same video. The major difference among these methods is the mapping strategy of linking still image detections to cross-frame box sequences. 

```[Seq-NMS]``` **[Seq-NMS]** links cross-frame bounding boxes iff their IoU is beyond a certain threshold and generate potential linkages across the entire clip. Then they propose a heuristic method for re-ranking bounding boxes called “Seq-NMS”.

```[TCNN]``` **[TCNN]** [14, 15] focus on tubelet rescoring. Tubelets are bounding boxes of an object over time. They apply an offline tracker to revisit the detection results and then associate still-image object detections around the tubelets. [15] presents a **re-scoring method** to improve the tubelets in terms of temporal consistency. Moreover, [14] proposes multi-context suppression (MCS) to **suppress false positive** detections and motion-guided propagation (MGP) to **recover false negatives**.

```[D&T]``` **[D&T]** D&T [5] is the first work to joint learn ROI tracker along with detector. The cross-frame tracker is used to boost the scores for positive boxes.

## Reference

[14] Kang, K., et al.: T-cnn: Tubelets with convolutional neural networks for object detection from videos. IEEE Transactions on Circuits and Systems for Video Technology (2017)

[15] Kang, K., Ouyang, W., Li: Object detection from video tubelets with convolutional neural networks. In: Proceedings of the IEEE Conference on CVPR. pp. 817–825 (2016)



